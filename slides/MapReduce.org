* Snakes on a Hadoop :slide:
  + PyCon 2014
  + Jim Blomo
  + https://jblomo.github.io/pycon-mrjob/slides/MapReduce.html#sec-1

* Let's Jump In :slide:
#+begin_src shell
git clone https://github.com/jblomo/pycon-mrjob.git
virtualenv venv
. venv/bin/activate
pip install -r requirements.txt
#+end_src
** Versions :notes:
   + All tests were done with Python 2.7
   + This is what is used on AWS EMR, so we're going to try to keep versions
     compatible.

* Agenda :slide:
  + What is MapReduce?
  + Writing MapReduce in mrjob
  + Exercise 1, user activity
  + Joins and Programmatic usage
  + Exercise 2, self-joins
  + Multi-step jobs & Protocols
  + Exercise 3, finding Yelp reviews with unique words
  + Exploring user-user similarity
  + Exercise 4, putting it all together
  + Deploying on AWS
** Sections :notes:
   + I will give some background, answer questions (35 minutes)
   + I'll introduce a script (5 minutes)
   + The exercise will have audience extending the script to get it working (15 minutes)
   + We'll go over an example solution (5 minutes)

* Interest? :slide:
  + What are you interested in?
  + What are you hoping to get out of this talk?
** Audience participation! :notes:
   + This will help focus this talk

* Run a Test :slide:
#+begin_src html
python venv/lib/python2.7/site-packages/mrjob/examples/mr_word_freq_count.py requirements.txt
#+end_src

* Yelp has a problem :slide:
  + 250+ GB of logs per day
  + Each GB takes 10 minutes to process
  + How long to handle a day's logs?
[[file:img/yelp-growth.png]]
** Too long :notes:
   + On a single machine 40+ hours!
   + If we really had only a single machine, we wouldn't be able to keep up!
   + Mistake can't be fixed in a day (billing especially important)

* Solution? :slide:animate:
  + Don't use one machine!
  + What are the new challenges?
  + Distributing data
  + Calculating overall statistics
  + Failures
** New Challenges :notes:
   + With many machines, how to they get access to the 100 GB of logs?
   + How do they coordinate who gets which section of logs?
   + How do we calculate the average?
   + What happens when one of the boxes dies?
     + Detecting failure (timeout waiting for data? Out of band?)
     + Decide who takes over the data

* Do It Yourself :slide:two_col:
  + There are many ways to deal with these challenges
  + Often, people would "roll" their own solutions depending on the problem
  + Google implemented a generic solution, shared idea
[[file:img/mapreduce-paper.png]]
** Dependencies :notes:
   + Did you have a super-computer?
   + What programming language were you using?
   + Type of problem being solved (working on graphs, or web logs, ...)

* Big Idea :slide:
  + Simplify, limit solution expression
  + Enable sophisticated implementation


  + Interface: Map() Reduce()
  + Implementation: Reliably run over 1000s of machines
** Really Big Idea :notes:
   + Limiting yourself to what can be expressed may seem like a loss
   + But it enables the implementation to handle the problems we talked about
   + And then can be used as understandable building blocks

* MapReduce :slide:
  + Map :: Extract a property to summarize over
  + Reduce :: Summarize all items with a particular propery


  + Simple: Each operation stateless
** Reading :notes:
   + MapReduce's main benefits are for running over many machines, fault
     tolerance
   + But we'll just practice on one machine
   + Then see how to run in the cloud

** Example :slide:
   + URL Shortener
   + How many actions have we seen?
   + Redirects: 450, Saves: 40, Loads: 60
*** Details :notes:
   + Redirects :: How many times have we expanded a short link to a long one?
   + Saves :: How many times have we saved a new URL?
   + Loads :: How many times have we just loaded the front page?
   + First :: So first step in MapReduce is what?

** Map :slide:
   + Input :: Key, Value
   + Output :: Keys, Values

** Map Example :slide:
   + Input Key :: Log line number
   + Input Value :: Log line text
   + Output Key :: Action
   + Output Value :: times this action has occurred on this line
*** Counts :notes:
   + Log line number is not helpful in our specific case
   + Log line text: we hope it is machine readable so we can accurately extract
     the action
   + It has datetime, cookie, action, etc.
   + How many times has this action occurred? 1

** Status? :slide:
#+begin_src text
load		1
save		1
redirect	1
redirect	1
load		1
redirect	1
load		1
save		1
redirect	1
#+end_src
*** Middle Step :notes:
   + From log lines, we've extracted the information out that we care about
   + The counts and the actions
   + Next step summarize
   + Next step after Map?

** Reduce :slide:
   + Input :: Key, Values
   + Output :: Keys, Values
*** Value*s* :notes:
   + Note: The input is values! Plural
   + Because we get a key and all of its associated values
   + Remind me: what are we trying to get out of this computation?
   + So what do you think the output keys are?
   + Values?

** Reduce Example :slide:
   + Input Key :: Action
   + Input Values :: Counts: =[1,1,1,1]=
   + Output Key :: Action
   + Output Value :: Total Count
*** Details :notes:
   + Action is *one of* load save redirect
   + To get total count, sum all of the counts

** Example Output :slide:
   + Output Key :: Action
   + Output Value :: Total Count
#+begin_src html
"redirect"  4
"save"      2
"load"      3
#+end_src

* Point? :slide:
  + A lot of work for counting!
  + More complex calculations can be done this way, eg. PageRank
  + Stateless constraint means it can be used across thousands of computers
** Details :notes:
   + By only looking at keys and values, can optimize a lot of backend work
   + Where to send the results?
   + What to do when a computer fails? (Just restart failed part)

** Implementation :slide:
#+begin_src text
load		1
save		1
redirect	1
redirect	1
load		1
redirect	1
load		1
save		1
redirect	1
#+end_src
** Intermediate :notes:
   + This was the situation after map
   + Keys all jumbled
   + What Hadoop does is sort them and distribute them to computers

** "Shuffle" :slide:
#+begin_src text
load		1
load		1
load		1
redirect	1
redirect	1
redirect	1
redirect	1
save		1
save		1
#+end_src
** Distribute :notes:
   + Now it is easy to distribute, and can handle all the =load= at once

** Inputs :slide:
   + MapReduce distributes computing power by distributing input
   + Input is distributed by splitting on lines (records)
   + You cannot depend on lines being "together" in MapReduce
*** Splitting Files :notes:
   + Image you have a lot of large log files, GB each
   + You'd like to let different machines work on the same file
   + Split file down the middle, well, at least on a newline
   + Enable two separate machines to work on the parts
   + You don't know what line came before this one
   + You don't know if you will process the next line
   + Only view is this line
   + Real life slightly more complicated, but mostly hacks around this

** Incorrect Log Style :slide:
   + URL Shortener logging
#+begin_src python
    app.logger.info("Handling request for " + cookie)
    ...
    # find redirect
    ...
    app.logger.info("Redirecting to " + destination)
#+end_src
    + Ability to associate the redirect with the
      cookie?
      + eg. which cookie had the most redirects?
*** No! :notes:
    + Must log everything on same line
    + One machine could have the "Handling request.." Another could have
      "Redirecting to..."
    + Collect it all, then log it

** Correct Log Style :slide:
   + Logging once
#+begin_src python
    log_data['cookie'] = cookie
    ...
    # find redirect
    log_data['action'] = 'redirect'
    app.logger.info(json.dumps(log_data))
#+end_src

* Data :slide:
  + [[http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/][Anonymous web data from www.microsoft.com]]
  + Contains information in CSV format
  + use mrjob MapReduce Framework to find answers
** Format :notes:

* [[http://packages.python.org/mrjob/][mrjob]] :slide:
#+begin_src html
from mrjob.job import MRJob

class MRWordCounter(MRJob):
    def mapper(self, key, line):
        for word in line.split():
            yield word, 1

    def reducer(self, word, occurrences):
        yield word, sum(occurrences)

if __name__ == '__main__':
    MRWordCounter.run()
#+end_src
    More documentation: http://pythonhosted.org/mrjob/
** Relation to HW :notes:
   + =mapper= takes keys and values
   + =reducer= takes the keys output by the mapper, and all relevant values
   + =split= takes a string and splits on spaces, giving words
   + =yield= essentially returns <key,value> pairs, but can be called more than
     once

** Output :slide:
#+begin_src bash
python code/top_pages.py msanon/anonymous-msweb.data.gz
no configs found; falling back on auto-configuration
creating tmp directory /tmp/top_pages.jim.20121116.052647.278066
...
reading from STDIN
writing to /tmp/top_pages.jim.20121116.052647.278066/step-0-mapper
Counters from step 1:
  (no counters found)
writing to /tmp/top_pages.jim.20121116.052647.278066/step-0-mapper-sorted
writing to /tmp/top_pages.jim.20121116.052647.278066/step-0-reducer
Counters from step 1:
  (no counters found)
Moving /tmp/top_pages.jim.20121116.052647.278066/step-0-reducer -> /tmp/top_pages.jim.20121116.052647.278066/output/part-00000
Streaming final output from /tmp/top_pages.jim.20121116.052647.278066/output
"1000"	912
"1001"	4451
"1002"	749
"1003"	2968
"1004"	8463
"1007"	865
...
#+end_src
** Running :notes:
   + Run with python
   + Output some debugging information while it is calculating
   + Finally, output results

* Pages with > 400 visits :slide:
  + Find pages (aka Vroots) with more than 400 visits
  + Start off with a template
** Demo :notes:
   + =csv_readline= takes in a CSV line, return a list of values

* *BREAK* :slide:
  + Please complete exercise 1
  + Fill in =code/top\_pages.py=

* Solution :slide:
  + =git checkout ex1=
  + yield the vroot ID and the number of times seen in that line (1)
  + sum the times seen, check threshold, yield pair

* Discussion: User Visits :slide:animate:
  + Can we calculate the number of visits per user?
  + No! User information on different line
  + Cannot assume linear processing

* Transform Data :slide:
  + MapReduce needs all information on one line
  + This data format has user information on different line than visit
  + Write single threaded program (not mrjob) to transform it
** Not covered :notes:
   + This is a little out of the scope of this tutorial
   + Just know that not all types of data are amenable to MapReduce, and
   Hadoop/mrjob specifically

* Most Common Title Words :slide:
  + Vroots (pages) have titles
  + What are the titles of the 10 most browsed nodes?

* Joins :slide:two_col:
  + Sometimes need to join against two data sets
  + In our case, it is the same file, different record types
  + mrjob can pass serializable objects between Map and Reduce steps

#+begin_src csv
A,1012,1,"Outlook Development","/outlookdev"
...
V,1012,1
#+end_src

* Running Programatically :slide:
  + We've been running in the command line
  + How to process the output of an MRJob inline
  + Testing, insert results into a database, do further processing on all of
  result

* MRJob Class :slide:
#+begin_src python
    mr_job = MRJobSubclass(args=[...])
    with mr_job.make_runner() as runner:
        runner.run()
        runner.stream_output()
#+end_src
** Example :notes:
   + =code/top\_titles.py=

* *BREAK* :slide:
  + Please complete exercise 2
  + Fill in =code/count\_titles.py=
  + Examine and run with =code/top\_titles.py=

* Solution :slide:
  + =git checkout ex2=
  + yield a tuple tagged with either 'A' or 'V'
  + When iterating through values, extract the data from the tagged tuple

* Yelp Data :slide:
  + Subset of data available in  =yelp/selected_reviews.json==
#+begin_src json
{"votes": {"funny": 3, "useful": 3, "cool": 2},
 "user_id": "dYtkphUrU7S2_bjif6k2uA",
 "review_id": "gjtWdiEMMfoOTCfdd3hPmA",
 "stars": 4,
 "date": "2009-04-24",
 "text": "Man, if these guys were trying to replicate a gringo bar in Mexico...",
 "type": "review",
 "business_id": "RqbSeoeqXTwts5pfhw7nJg"}
#+end_src
** More Data :notes:
   + You may run with more data by downloading and extracting from
     http://www.yelp.com/dataset_challenge

* Multi-Step :slide:
  + Not all computations can be done in a single MapReduce step
  + Map Input: <key, value>
  + Reducer Output: <key, value>
  + Compose MapReduce steps!
** Output as Input :notes:
   + The output of one MapReduce job can be used as the input to another

** Examples :slide:
   + PageRank: Multiple steps till solution converges
   + Multi-level summaries
** PageRank :notes:
   + PageRank is an algorithm for calculating the important of a page
   + But it depends on the importance of every page pointing to it!
   + So iteratively calculate the important of all pages
   + Find average presidential donations by candidate, then normalize averages

* Unique Review :slide:animate:
  + Count unique words per review
  + Map Input: <line number, text>
  + Map Output: <word, review\_id>
  + Reducer Input: <word, [review\_ids]>
  + Reducer Output: <review\_id, 1> if the word is unique
** Questions :notes:
   + For our purposes, what is always the mapper input?
   + What feature do we want to calculate first?
   + Given this mapper output, what *must* the reducer input be?
   + What property about a review are we interested in?

** Step 2: Count Unique Words :slide:animate:
   + Map Input: <review\_id, 1>
   + Map Output: <review\_id, 1>
   + Reducer Input: <review\_id, [1,1,...]>
   + Reducer Output: <review\_id, sum>
** Questions :notes:
   + Given the reducer output, what *must* the mapper input be (for chained
     MapReduce steps)
   + What do we want to group by?
   + Given this mapper output, what *must* the reducer input be?
   + What are we calculating?

* INPUT_PROTOCOL :slide:
  + Previous examples manually parsed line
  + mrjob provides "protocols" to automatically parse and recover from error
  + Specify the class desired to parse the lines
** Examples :notes:
   + We're using a JSON object per line. Other options include JSON key-values,
   or writing your own
   + Instead of raw text, =record= will now be a Python dict

* *BREAK* :slide:
  + Please complete exercise 3
  + Fill in =code/unique\_review.py=
  + Run with data in =yelp/selected_reviews.json=

* Solution :slide:
  + Extract words from the review comment field
  + Group them by word, keep those that are unique
  + Group them by review, return sum

* Jaccard Coefficient :slide:
  + Commonly used for calculating set similarity
  + =|intersection| / |union|=
  + "Jim likes pizza" | "Shreyas likes pizza"
** Jaccard :notes:
   1. Break up into a set
   1. calculate # in intersection
   1. calculate # in union
   1. divide

* Write user\_similarity.py :slide:
  + Find users >= 0.5 similarity
  + User Similarity: Jaccard similarity of businesses reviewed
  + {BizA, BizB, BizC} ~ {BizF, BizB, BizG}

* *BREAK* :slide:
  + Please complete exercise 4
  + Fill in =code/user\_similarity.py=
  + Run with data in =yelp/selected_reviews.json=
  + Hint: Users with > 0 similarity will share at least 1 business

* Solution :slide:
  + Map user to list of businesses reviewed
  + Map business to list of user descriptions
  + Find users with similarity
  + De-duplicate
** Efficiency :notes:
   + We do duplicate work, but the trade-off is parallelism in the best case
   (not many users are overlapping with each other for dozens of reviews)

* EMR :slide:
  + We've been using local mode
  + Effective for debugging, small amounts of data
  + What about actually processing Big Data?

* Elastic MapReduce :slide:
  + Service by Amazon
  + Installs and sets up Hadoop cluster
  + Ability to run scripts at setup

* mrjob provides :slide:
  + Encode standard settings as config file or arguments
  + Uploads and runs bootstrap commands w/ local integration
  + Transparent inter-job bookkeeping
  + Tracking, error collection, job sharing

* How To :slide:
#+begin_src shell
python code/unique_review.py -r emr -c mrjob.conf s3://i290-jblomo/data/selected_reviews.json.gz
#+end_src
  + Simply specify =-r emr=

* Settings :slide:
  + Instance types
  + EMR keys
  + Location for logs
  + SSH key
  + Bootstrap behavior

* Bootstrap :slide:
  + use of # for file uploads
  + location of runs (different than raw bootstrap commands)
  + installation of python packages

* Book Keeping :slide:
  + --output-dir s3://output-bucket/path/
  + --no-output
  + intermediate results stored on HDFS

* Tracking :slide:two_col:
  + mrjob opens an SSH tunnel to the master node for Hadoop stats
  + On job failure, will download logs and use heuristics to find likely error
  [[file:img/HadoopUI.png]]

* EMR Pricing :slide:
  + Job flows are billed hourly, rounding *up*
[[file:img/jobflow-no-pooling.png]]
** Typical usage :notes:
   + Spin up a job flow, when finished, kill it
   + Remaining time and end of last hour is wasted
   + Billed for it, but not used

** Job Flow Pooling :slide:
   + Don't end job flow after job, share remaining time
[[file:img/jobflow-pooling.png]]
   + New jobs can wait N minutes for an idle flow
*** Pooling :notes:
   + Billed on the hour, so don't give up early!
   + Only shut down idle job flows at the end of the hour
   + If another job comes up, reuse idle flow
   + Also makes job faster to spin up BONUS for developers

* mrjob Workflow :slide:
  + Develop locally on subset of data
  + Run on EMR on all data located in S3 bucket
  + Either save output in S3, or stream locally

* Thank You! :slide:
  + Jim Blomo, Engineering Manager Yelp
  + @jimblomo

#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="production/common.css" />
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="production/screen.css" media="screen" />
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="production/projection.css" media="projection" />
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="production/color-blue.css" media="projection" />
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="production/presenter.css" media="presenter" />
#+HTML_HEAD_EXTRA: <link href='http://fonts.googleapis.com/css?family=Lobster+Two:700|Yanone+Kaffeesatz:700|Open+Sans' rel='stylesheet' type='text/css'>

#+BEGIN_HTML
<script type="text/javascript" src="production/org-html-slideshow.js"></script>
#+END_HTML

# Local Variables:
# org-export-html-style-include-default: nil
# org-export-html-style-include-scripts: nil
# buffer-file-coding-system: utf-8-unix
# End:
